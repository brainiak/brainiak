{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event segmentation and alignment in fMRI data\n",
    "\n",
    "## Overview\n",
    "Humans tend to segment continuous perceptual experiences into discrete events. We have devised this notebook to capture the neural representations of this \"chunking\" process using a Hidden Markov Models (HMM). The notebook uses real data from naturalistic movie viewing and recall, suggesting ways to align derived neural event states with subjective annotations of event boundaries and with the recall time series.\n",
    "\n",
    "## Annotated Bibliography\n",
    "1. Baldassano, C., Chen, J., Zadbood, A., Pillow, J. W., Hasson, U., & Norman, K. A. (2017). Discovering event structure in continuous narrative perception and memory. *Neuron*, 95(3), 709–721.e5. [`link`](https://doi.org/10.1016/j.neuron.2017.06.041) *Describes and validates the event segmentation method, and applies it to perception and recall data from multiple experiments.*\n",
    "\n",
    "2. Baldassano, C., Hasson, U., & Norman, K. A. (2018). Representation of real-world event schemas during narrative perception. *Journal of Neuroscience*, 38(45), 9689–9699. [`link`](https://doi.org/10.1523/JNEUROSCI.0251-18.2018) *Uses the event segmentation model to find common event structure among narratives with a shared schematic script.*\n",
    "\n",
    "3. Ben-Yakov, A., & Henson, R. N. (2018). The hippocampal film editor: sensitivity and specificity to event boundaries in continuous experience. *Journal of Neuroscience*, 38(47), 10057–10068. [`link`](https://doi.org/10.1523/JNEUROSCI.0524-18.2018) *Further studies the relationship between the event boundaries produced by the event segmentation model, human-annotated boundaries, and hippocampal responses.*\n",
    "\n",
    "4. Silva, M., Baldassano, C., & Fuentemilla, L. (2019). Rapid memory reactivation at movie event boundaries promotes episodic encoding. *Journal of Neuroscience*, 39(43), 8538–8548. [`link`](https://doi.org/10.1523/JNEUROSCI.0360-19.2019) *Applies the event segmentation model to EEG signals collected while subjects were watching a movie.*\n",
    "\n",
    "5. Antony, J. W., Hartshorne, T. H., Pomeroy, K., Gureckis, T. M., Hasson, U., McDougle, S. D., & Norman, K. A. (2020). Behavioral, physiological, and neural signatures of surprise during naturalistic sports viewing. *Neuron*. [`link`](https://doi.org/10.1016/j.neuron.2020.10.029) *Uses the event segmentation model to relate the number and timing of event boundaries in neural signals  to the degree of surprise elicited in basketball games.*\n",
    "\n",
    "## Table of Contents\n",
    "* [Loading data](#data)\n",
    "* [Finding event boundaries during perception](#perception)\n",
    "* [Comparing model and human-labeled boundaries](#comparing)\n",
    "* [Aligning movie and recall data](#aligning)\n",
    "* [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "import os    \n",
    "import glob\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from brainiak.eventseg.event import EventSegment\n",
    "from scipy.stats import norm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "smallsize=14; mediumsize=16; largesize=18\n",
    "plt.rc('xtick', labelsize=smallsize); plt.rc('ytick', labelsize=smallsize); plt.rc('legend', fontsize=mediumsize)\n",
    "plt.rc('figure', titlesize=largesize); plt.rc('axes', labelsize=mediumsize); plt.rc('axes', titlesize=mediumsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data<a id='data'></a>\n",
    "This tutorial will use data from the first run of the Sherlock dataset [(Chen et al. 2017)](https://doi.org/10.1038/nn.4450), masked to only include the Angular Gyrus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Sherlock_AG_movie.npy'):\n",
    "    !wget https://ndownloader.figshare.com/files/22927253 -O Sherlock_AG_movie.npy\n",
    "if not os.path.exists('Sherlock_AG_recall.npy'):\n",
    "    !wget https://ndownloader.figshare.com/files/22927256 -O Sherlock_AG_recall.npy\n",
    "\n",
    "movie = np.load('Sherlock_AG_movie.npy')\n",
    "recall = np.load('Sherlock_AG_recall.npy')\n",
    "movie_group = np.mean(movie, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding event boundaries during perception<a id='perception'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up: Event structure in activity patterns\n",
    "\n",
    "Before applying any model, a good first step is to plot the correlation between activity patterns for each pair of timepoints during the movie. In this dataset, this shows blocks along the diagonal, which indicates that activity patterns are remaining stable for periods of tens of timepoints. This is the kind of structure that the HMM will be looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(np.corrcoef(movie_group))\n",
    "plt.xlabel('Timepoint')\n",
    "plt.ylabel('Timepoint')\n",
    "plt.colorbar()\n",
    "plt.title('Spatial pattern correlation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the HMM\n",
    "\n",
    "To use an HMM to find both the event timings and the patterns corresponding to each event, we can use the EventSegment class from the brainiak toolbox. We need to specify the number of events, which here we set to 29 (corresponding to the number of boundaries typically annotated by human subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_HMM = EventSegment(n_events = 29)\n",
    "movie_HMM.fit(movie_group);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit produces:\n",
    "* The log-likelihood (measuring overall model fit) over training. (Note that the log-likelihood on held-out test data is often a better measure of model quality - see below).\n",
    "* The mean voxel pattern for each event. Here we show only 1% of the voxels since the ROI is large.\n",
    "* A matrix showing the probability of being in each event at each timepoint. We can use this to derive the most likely timepoints where boundaries occur, and plot these on top of the timepoint similarity matrix for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the log-likelihood (measuring overall model fit)\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(movie_HMM.ll_)\n",
    "plt.title('Log likelihood during training')\n",
    "plt.xlabel('Model fitting steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting mean activity in each event for some example voxels\n",
    "plt.figure(figsize = (12, 4))\n",
    "example_vox = np.arange(0,movie_HMM.event_pat_.shape[0],100)\n",
    "plt.imshow(movie_HMM.event_pat_[example_vox,:], aspect='auto')\n",
    "plt.xlabel('Event number')\n",
    "plt.ylabel('Voxels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probability of being in each event at each timepoint\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.matshow(movie_HMM.segments_[0].T, aspect='auto')\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.colorbar()\n",
    "plt.title('Event probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify event boundaries as timepoints when max probability switches events\n",
    "event_bounds = np.where(np.diff(np.argmax(movie_HMM.segments_[0], axis = 1)))[0]\n",
    "nTRs = movie_group.shape[0]\n",
    "\n",
    "\n",
    "# Plot boundaries as boxes on top of timepoint correlation matrix\n",
    "def plot_tt_similarity_matrix(ax, data_matrix, bounds, n_TRs, title_text):\n",
    "    \n",
    "    ax.imshow(np.corrcoef(data_matrix), cmap = 'viridis')\n",
    "    ax.set_title(title_text)\n",
    "    ax.set_xlabel('TR')\n",
    "    ax.set_ylabel('TR')\n",
    "    \n",
    "    # plot the boundaries \n",
    "    bounds_aug = np.concatenate(([0], bounds, [n_TRs]))\n",
    "    \n",
    "    for i in range(len(bounds_aug) - 1):\n",
    "        rect = patches.Rectangle(\n",
    "            (bounds_aug[i], bounds_aug[i]),\n",
    "            bounds_aug[i+1] - bounds_aug[i],\n",
    "            bounds_aug[i+1] - bounds_aug[i],\n",
    "            linewidth = 2, edgecolor = 'w',facecolor = 'none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize = (10,8))\n",
    "title_text = '''\n",
    "Overlay the HMM-predicted event boundaries\n",
    "on top of the TR-TR correlation matrix\n",
    "'''\n",
    "plot_tt_similarity_matrix(ax, movie_group, event_bounds, nTRs, title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the number of events with the HMM\n",
    "\n",
    "What if we don't want to prespecify the number of events, but instead want to determine the number of events from the data? One way to determine the best number of events is to fit the model on a training set and then test the model fit on independent subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_array = np.arange(20, 61, 10)\n",
    "test_ll = np.zeros(len(k_array))\n",
    "\n",
    "for i, k in enumerate(k_array):\n",
    "    print('Trying %d events' % k)\n",
    "    \n",
    "    print('   Fitting model on training subjects...')\n",
    "    movie_train = np.mean(movie[:8], axis = 0)\n",
    "    movie_HMM = EventSegment(k)\n",
    "    movie_HMM.fit(movie_train)\n",
    "    \n",
    "    print('   Testing model fit on held-out subjects...')\n",
    "    movie_test = np.mean(movie[8:], axis = 0)\n",
    "    _, test_ll[i] = movie_HMM.find_events(movie_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_array, test_ll)\n",
    "plt.xlabel('Number of events')\n",
    "plt.ylabel('Log-likelihood')\n",
    "\n",
    "movie_dur = nTRs * 1.5  # Data acquired every 1.5 seconds\n",
    "secax = plt.gca().secondary_xaxis('top',\n",
    "                                  functions=(lambda x: movie_dur / (x + sys.float_info.epsilon),\n",
    "                                             lambda x: movie_dur / (x + sys.float_info.epsilon)))\n",
    "secax.set_xlabel('Average event length (sec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal segmentation with the HMM\n",
    "Since 40 events maximized the test log-likelihood, we'll generate two versions of HMM boundaries using 40 events. In addition to the \"vanilla\" HMM, we'll run an HMM with more flexibility during fitting (allowing for split-merge operations). This is slower (and so should usually only be used for generating a final segmentation), but can produce better fits if events are very uneven in duration. We will use these segmentations below for comparison with human labeled event boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting HMM with 40 events...')\n",
    "HMM40 = EventSegment(n_events = 40)\n",
    "HMM40.fit(movie_group)\n",
    "HMM40_bounds = np.where(np.diff(np.argmax(HMM40.segments_[0], axis = 1)))[0]\n",
    "\n",
    "print('Fitting split-merge HMM with 40 events...')\n",
    "HMM40_SM = EventSegment(n_events = 40, split_merge = True)\n",
    "HMM40_SM.fit(movie_group)\n",
    "HMM40_SM_bounds = np.where(np.diff(np.argmax(HMM40_SM.segments_[0], axis = 1)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing model and human-labeled boundaries<a id='comparing'></a>\n",
    "\n",
    "We can also quantitatively compare the event boundaries between different models, or between a model and human-labeled event boundaries. Because there is some ambiguity in both the stimulus and the model about exactly which timepoint the transition occurs at, we will count two boundaries as being a \"match\" if they are within 3 TRs (4.5 seconds) of each other.\n",
    "\n",
    "To determine whether the match is statistically significant, we generate permuted versions of the boundaries as a null model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timepoints of event boundaries annotated by human raters\n",
    "human_bounds = [\n",
    "    26, 35, 56, 72, 86, 108, 131, 143, 157, 173, 192, 204, \n",
    "    226, 313, 362, 398, 505, 526, 533, 568, 616, 634, 678,\n",
    "    696, 747, 780, 870, 890\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes fraction of ground truth bounds that are covered by a set of proposed bounds\n",
    "# Returns z score relative to a null distribution via permutation\n",
    "def match_z(proposed_bounds, gt_bounds, num_TRs):\n",
    "    nPerm = 1000\n",
    "    threshold = 3\n",
    "    np.random.seed(0)\n",
    "\n",
    "    gt_lengths = np.diff(np.concatenate(([0],gt_bounds,[num_TRs])))\n",
    "    match = np.zeros(nPerm + 1)\n",
    "    for p in range(nPerm + 1):\n",
    "        gt_bounds = np.cumsum(gt_lengths)[:-1]\n",
    "        for b in gt_bounds:\n",
    "            if np.any(np.abs(proposed_bounds - b) <= threshold):\n",
    "                match[p] += 1\n",
    "        match[p] /= len(gt_bounds)\n",
    "        gt_lengths = np.random.permutation(gt_lengths)\n",
    "    \n",
    "    return (match[0]-np.mean(match[1:]))/np.std(match[1:])\n",
    "\n",
    "z = [match_z(HMM40_bounds, human_bounds, nTRs),\n",
    "     match_z(HMM40_SM_bounds, human_bounds, nTRs)]\n",
    "\n",
    "print('HMM40:    z = %f, p = %f' % (z[0], norm.sf(z[0])))\n",
    "print('HMM_SM40: z = %f, p = %f' % (z[1], norm.sf(z[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning movie and recall data<a id='aligning'></a>\n",
    "\n",
    "A simple model of free recall is that a subject will revisit the same sequence of events experienced during perception, but the lengths of the events will not be identical between perception and recall. We use the same fit function as for a single dataset, but now we pass in both the movie and recall datasets in a list. We assume the two datasets have shared event transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_recall_HMM = EventSegment(40)\n",
    "movie_recall_HMM.fit([movie_group, recall]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(movie_recall_HMM.segments_[0] @ movie_recall_HMM.segments_[1].T)\n",
    "plt.xlabel('Timepoints during recall')\n",
    "plt.ylabel('Timepoints during movie')\n",
    "plt.colorbar()\n",
    "plt.title('Prob of being in the same event');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary<a id='summary'></a>\n",
    "Using the HMM, we first captured neural states corresponding with the naturalistic segmentation of events. Then, to verify that these states aligned with subjective event perception, we aligned their boundaries with event boundary annotations from an independent group of subjects. Finally, we showed that processes such as free recall, which feature similar transition structures but may be compressed or expanded in time, can be aligned to this perceptual HMM \"template\", broadening the scope of future research questions that can be addressed with this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
