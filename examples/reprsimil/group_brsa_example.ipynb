{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demo shows how to use the Group Bayesian Representational Similarity Analysis (GBRSA) method in brainiak with a simulated dataset.\n",
    "\n",
    "## Note that although the name has \"group\", it is also suitable for analyzing data of a single participant\n",
    "\n",
    "## When you apply this tool to real fMRI data, it is required that the data of each participant to be motion corrected. If multiple runs are acquired for each participant, they should be spatially aligned. You might want to do slice-timing correction. \n",
    "\n",
    "## You will need to have the mask of the Region of Interest (ROI) ready (defined anatomically or by independent tasks, which is up to you). nilearn provides tools to extract signal from mask. You can refer to http://nilearn.github.io/manipulating_images/manipulating_images.html\n",
    "\n",
    "## When analyzing an ROI of hundreds to thousands voxels, it is expected to be faster than the non-group version BRSA (refer to the other example). The reason is that GBRSA marginalize the SNR and AR(1) coefficient parameters of each voxel by numerical integration, thus eliminating hundreds to thousands of free parameters and reducing computation. However, if you are doing searchlight analysis with tens of voxels in each searchlight, it is possible that BRSA is faster.\n",
    "\n",
    "## GBRSA and BRSA might not return exactly the same result. Which one is more accurate might depend on the parameter choice, as well as the property of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load some package which we will use in this demo.\n",
    "If you see error related to loading any package, you can install that package. For example, if you use Anaconda, you can use \"conda install matplotlib\" to install matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import scipy.spatial.distance as spdist\n",
    "import numpy as np\n",
    "from brainiak.reprsimil.brsa import GBRSA\n",
    "import brainiak.utils.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "np.random.seed(10)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You might want to keep a log of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    filename='gbrsa_example.log',\n",
    "    format='%(relativeCreated)6d %(threadName)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We want to simulate some data in which each voxel responds to different task conditions differently, but following a common covariance structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an example design matrix.\n",
    "The user should prepare their design matrix with their favorate software, such as using 3ddeconvolve of AFNI, or using SPM or FSL.\n",
    "The design matrix reflects your belief of how fMRI signal should respond to a task (if a voxel does respond).\n",
    "The common assumption is that a neural event that you are interested in will elicit a slow hemodynamic response in some voxels. The response peaks around 4-6 seconds after the event onset and dies down more than 12 seconds after the event. Therefore, typically you convolve a time series A, composed of delta (stem) functions reflecting the time of each neural event belonging to the same category (e.g. all trials in which a participant sees a face), with a hemodynamic response function B, to form the hypothetic response of any voxel to such type of neural event.\n",
    "For each type of event, such a convoluted time course can be generated. These time courses, put together, are called design matrix, reflecting what we believe a temporal signal would look like, if it exists in any voxel.\n",
    "Our goal is to figure out how the (spatial) response pattern of a population of voxels (in an Region of Interest, ROI) are similar or disimilar to different types of tasks (e.g., watching face vs. house, watching different categories of animals, different conditions of a cognitive task). So we need the design matrix in order to estimate the similarity matrix we are interested.\n",
    "\n",
    "We can use the utility called ReadDesign from brainiak.utils to read a design matrix generated from AFNI. For design matrix saved as Matlab data file by SPM or or other toolbox, you can use scipy.io.loadmat('YOURFILENAME') and extract the design matrix from the dictionary returned. Basically, the Bayesian RSA in this toolkit just needs a numpy array which is in size of {time points} * {condition}\n",
    "You can also generate design matrix using the function gen_design which is in brainiak.utils. It takes in (names of) event timing files in AFNI or FSL format (denoting onsets, duration, and weight for each event belongning to the same condition) and outputs the design matrix as numpy array.\n",
    "\n",
    "In typical fMRI analysis, some nuisance regressors such as head motion, baseline time series and slow drift are also entered into regression. In using our method, you should not include such regressors into the design matrix, because the spatial spread of such nuisance regressors might be quite different from the spatial spread of task related signal. Including such nuisance regressors in design matrix might influence the pseudo-SNR map, which in turn influence the estimation of the shared covariance matrix. But you may include motion time course in the nuisance parameter.\n",
    "\n",
    "### We concatenate the design matrix by 2 to 3 times, mimicking 2 to 3 runs of identical timing\n",
    "### Note that different subjects do not have to have the same number of voxels or time points. The timing of the task conditions of them can also differ. The simulation below reflects this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subj = 5\n",
    "n_run = np.random.random_integers(2, 4, n_subj)\n",
    "ROI_edge = np.random.random_integers(20, 40, n_subj)\n",
    "# We simulate \"ROI\" of a square shape\n",
    "design = [None] * n_subj\n",
    "for subj in range(n_subj):\n",
    "    design[subj] = utils.ReadDesign(fname=\"example_design.1D\")\n",
    "    design[subj].n_TR = design[subj].n_TR * n_run[subj]\n",
    "    design[subj].design_task = np.tile(design[subj].design_task[:,:-1],\n",
    "                                 [n_run[subj], 1])\n",
    "    # The last \"condition\" in design matrix\n",
    "    # codes for trials subjects made an error.\n",
    "    # We ignore it here.\n",
    "n_C = np.size(design[0].design_task, axis=1)\n",
    "# The total number of conditions.\n",
    "n_V = [int(roi_e**2) for roi_e in ROI_edge]\n",
    "# The total number of simulated voxels\n",
    "n_T = [d.n_TR for d in design]\n",
    "# The total number of time points,\n",
    "# after concatenating all fMRI runs\n",
    "fig = plt.figure(num=None, figsize=(12, 3),\n",
    "                 dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(design[0].design_task)\n",
    "plt.ylim([-0.2, 0.4])\n",
    "plt.title('hypothetic fMRI response time courses '\n",
    "          'of all conditions for one subject\\n'\n",
    "         '(design matrix)')\n",
    "plt.xlabel('time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate data: noise + signal\n",
    "### First, we start with noise, which is Gaussian Process in space and AR(1) in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_bot = 0.5\n",
    "noise_top = 1.5\n",
    "noise_level = [None] * n_subj\n",
    "noise = [None] * n_subj\n",
    "rho1 = [None] * n_subj\n",
    "for subj in range(n_subj):\n",
    "    noise_level[subj] = np.random.rand(n_V[subj]) * \\\n",
    "        (noise_top - noise_bot) + noise_bot\n",
    "    # The standard deviation of the noise is in the range of [noise_bot, noise_top]\n",
    "    # In fact, we simulate autocorrelated noise with AR(1) model. So the noise_level reflects\n",
    "    # the independent additive noise at each time point (the \"fresh\" noise)\n",
    "\n",
    "# AR(1) coefficient\n",
    "rho1_top = 0.8\n",
    "rho1_bot = -0.2\n",
    "for subj in range(n_subj):\n",
    "    rho1[subj] = np.random.rand(n_V[subj]) \\\n",
    "        * (rho1_top - rho1_bot) + rho1_bot\n",
    "\n",
    "noise_smooth_width = 10.0\n",
    "dist2 = [None] * n_subj\n",
    "for subj in range(n_subj):\n",
    "    coords = np.mgrid[0:ROI_edge[subj], 0:ROI_edge[subj], 0:1]\n",
    "    coords_flat = np.reshape(coords,[3, n_V[subj]]).T\n",
    "    dist2[subj] = spdist.squareform(spdist.pdist(coords_flat, 'sqeuclidean'))\n",
    "\n",
    "    # generating noise\n",
    "    K_noise = noise_level[subj][:, np.newaxis] \\\n",
    "        * (np.exp(-dist2[subj] / noise_smooth_width**2 / 2.0) \\\n",
    "           + np.eye(n_V[subj]) * 0.1) * noise_level[subj]\n",
    "    # We make spatially correlated noise by generating\n",
    "    # noise at each time point from a Gaussian Process\n",
    "    # defined over the coordinates.\n",
    "    L_noise = np.linalg.cholesky(K_noise)\n",
    "    noise[subj] = np.zeros([n_T[subj], n_V[subj]])\n",
    "    noise[subj][0, :] = np.dot(L_noise, np.random.randn(n_V[subj]))\\\n",
    "        / np.sqrt(1 - rho1[subj]**2)\n",
    "    for i_t in range(1, n_T[subj]):\n",
    "        noise[subj][i_t, :] = noise[subj][i_t - 1, :] * rho1[subj] \\\n",
    "            + np.dot(L_noise,np.random.randn(n_V[subj]))\n",
    "    # For each voxel, the noise follows AR(1) process:\n",
    "    # fresh noise plus a dampened version of noise at\n",
    "    # the previous time point.\n",
    "    # In this simulation, we also introduced spatial smoothness resembling a Gaussian Process.\n",
    "    # Notice that we simulated in this way only to introduce spatial noise correlation.\n",
    "    # This does not represent the assumption of the form of spatial noise correlation in the model.\n",
    "    # Instead, the model is designed to capture structured noise correlation manifested\n",
    "    # as a few spatial maps each modulated by a time course, which appears as spatial noise correlation. \n",
    "plt.pcolor(K_noise)\n",
    "plt.colorbar()\n",
    "plt.xlim([0, ROI_edge[-1] * ROI_edge[-1]])\n",
    "plt.ylim([0, ROI_edge[-1] * ROI_edge[-1]])\n",
    "plt.title('Spatial covariance matrix of noise\\n of the last participant')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 2), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "plt.plot(noise[-1][:, 0])\n",
    "plt.title('noise in an example voxel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we simulate signals, assuming the magnitude of response to each condition follows a common covariance matrix. \n",
    "#### Note that Group Bayesian Representational Similarity Analysis (GBRSA) does not impose Gaussian Process prior on log(SNR) as BRSA does, for two reasons: (1) computational speed, (2) we numerically marginalize SNR for each voxel in GBRSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's keep in mind of the pattern of the ideal covariance / correlation below and see how well BRSA can recover their patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal covariance matrix\n",
    "ideal_cov = np.zeros([n_C, n_C])\n",
    "ideal_cov = np.eye(n_C) * 0.6\n",
    "ideal_cov[8:12, 8:12] = 0.6\n",
    "for cond in range(8, 12):\n",
    "    ideal_cov[cond,cond] = 1\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(ideal_cov)\n",
    "plt.colorbar()\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('ideal covariance matrix')\n",
    "plt.show()\n",
    "\n",
    "std_diag = np.diag(ideal_cov)**0.5\n",
    "ideal_corr = ideal_cov / std_diag / std_diag[:, None]\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(ideal_corr)\n",
    "plt.colorbar()\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('ideal correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following, pseudo-SNR is generated from a Gaussian Process defined on a \"square\" ROI, just for simplicity of code\n",
    "#### Notice that GBRSA does not make assumption of smoothness of SNR, so it won't utilize this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_full = np.linalg.cholesky(ideal_cov)        \n",
    "\n",
    "# generating signal\n",
    "snr_level = np.random.rand(n_subj) * 0.6 + 0.4\n",
    "# Notice that accurately speaking this is not SNR.\n",
    "# The magnitude of signal depends not only on beta but also on x.\n",
    "# (noise_level*snr_level)**2 is the factor multiplied\n",
    "# with ideal_cov to form the covariance matrix from which\n",
    "# the response amplitudes (beta) of a voxel are drawn from.\n",
    "\n",
    "tau = np.random.rand(n_subj) * 0.8 + 0.2\n",
    "# magnitude of Gaussian Process from which the log(SNR) is drawn\n",
    "smooth_width = np.random.rand(n_subj) * 5.0 + 3.0\n",
    "# spatial length scale of the Gaussian Process, unit: voxel\n",
    "inten_kernel = np.random.rand(n_subj) * 4.0 + 2.0\n",
    "# intensity length scale of the Gaussian Process\n",
    "# Slightly counter-intuitively, if this parameter is very large,\n",
    "# say, much larger than the range of intensities of the voxels,\n",
    "# then the smoothness has much small dependency on the intensity.\n",
    "\n",
    "Y = [None] * n_subj\n",
    "snr = [None] * n_subj\n",
    "signal = [None] * n_subj\n",
    "betas_simulated = [None] * n_subj\n",
    "inten = [None] * n_subj\n",
    "for subj in range(n_subj):\n",
    "    inten[subj] = np.random.rand(n_V[subj]) * 20.0\n",
    "    # For simplicity, we just assume that the intensity\n",
    "    # of all voxels are uniform distributed between 0 and 20\n",
    "    # parameters of Gaussian process to generate pseuso SNR\n",
    "    # For curious user, you can also try the following commond\n",
    "    # to see what an example snr map might look like if the intensity\n",
    "    # grows linearly in one spatial direction\n",
    "\n",
    "\n",
    "    inten_tile = np.tile(inten[subj], [n_V[subj], 1])\n",
    "    inten_diff2 = (inten_tile - inten_tile.T)**2\n",
    "\n",
    "    K = np.exp(-dist2[subj] / smooth_width[subj]**2 / 2.0 \n",
    "               - inten_diff2 / inten_kernel[subj]**2 / 2.0) * tau[subj]**2 \\\n",
    "        + np.eye(n_V[subj]) * tau[subj]**2 * 0.001\n",
    "    # A tiny amount is added to the diagonal of\n",
    "    # the GP covariance matrix to make sure it can be inverted\n",
    "    L = np.linalg.cholesky(K)\n",
    "    snr[subj] = np.exp(np.dot(L, np.random.randn(n_V[subj]))) * snr_level[subj]\n",
    "    sqrt_v = noise_level[subj] * snr[subj]\n",
    "    betas_simulated[subj] = np.dot(L_full, np.random.randn(n_C, n_V[subj])) * sqrt_v\n",
    "    signal[subj] = np.dot(design[subj].design_task, betas_simulated[subj])\n",
    "\n",
    "\n",
    "    Y[subj] = signal[subj] + noise[subj] + inten[subj]\n",
    "    # The data to be fed to the program.\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(np.reshape(snr[0], [ROI_edge[0], ROI_edge[0]]))\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('pseudo-SNR in a square \"ROI\" \\nof participant 0')\n",
    "plt.show()\n",
    "\n",
    "snr_all = np.concatenate(snr)\n",
    "idx = np.argmin(np.abs(snr_all - np.median(snr_all)))\n",
    "median_subj = np.min(np.where(idx - np.cumsum(n_V) < 0))\n",
    "\n",
    "idx = idx - np.cumsum(np.concatenate([[0], n_V]))[median_subj]\n",
    "# choose a voxel of medium level SNR.\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "noise_plot, = plt.plot(noise[median_subj][:,idx],'g')\n",
    "signal_plot, = plt.plot(signal[median_subj][:,idx],'b')\n",
    "plt.legend([noise_plot, signal_plot], ['noise', 'signal'])\n",
    "plt.title('simulated data in an example voxel'\n",
    "          ' with pseudo-SNR of {} in participant {}'.format(snr[median_subj][idx], median_subj))\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "data_plot, = plt.plot(Y[median_subj][:,idx],'r')\n",
    "plt.legend([data_plot], ['observed data of the voxel'])\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "idx = np.argmin(np.abs(snr_all - np.max(snr_all)))\n",
    "highest_subj = np.min(np.where(idx - np.cumsum(n_V) < 0))\n",
    "idx = idx - np.cumsum(np.concatenate([[0], n_V]))[highest_subj]\n",
    "# display the voxel of the highest level SNR.\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "noise_plot, = plt.plot(noise[highest_subj][:,idx],'g')\n",
    "signal_plot, = plt.plot(signal[highest_subj][:,idx],'b')\n",
    "plt.legend([noise_plot, signal_plot], ['noise', 'signal'])\n",
    "plt.title('simulated data in the voxel with the highest'\n",
    "          ' pseudo-SNR of {} in subject {}'.format(snr[highest_subj][idx], highest_subj))\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "data_plot, = plt.plot(Y[highest_subj][:,idx],'r')\n",
    "plt.legend([data_plot], ['observed data of the voxel'])\n",
    "plt.xlabel('time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reason that the pseudo-SNRs in the example voxels are not too small, while the signal looks much smaller is because we happen to have low amplitudes in our design matrix. The true SNR depends on both the amplitudes in design matrix and the pseudo-SNR. Therefore, be aware that pseudo-SNR does not directly reflects how much signal the data have, but rather a map indicating the relative strength of signal in differerent voxels.\n",
    "#### When you have multiple runs, the noise won't be correlated between runs. Therefore, you should tell BRSA when is the onset of each scan. \n",
    "#### Note that the data (variable Y above) you feed to BRSA is the concatenation of data from all runs along the time dimension, as a 2-D matrix of time x space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_onsets = [np.int32(np.linspace(0, design[i].n_TR,num=n_run[i] + 1)[: -1]) for i in range(n_subj)]\n",
    "print('scan onsets: {}'.format(scan_onsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Group Bayesian RSA to our simulated data\n",
    "\n",
    "\n",
    "The nuisance regressors in typical fMRI analysis (such as head motion signal) are replaced by principal components estimated from residuals after subtracting task-related response. `n_nureg` tells the model how many principal components to keep from the residual as nuisance regressors, in order to account for spatial correlation in noise. When it is set to None and `auto_nuisance=True`, this number will be estimated automatically by an algorithm of Gavish & Dohono 2014. If you prefer not using this approach based on principal components of residuals, you can set `auto_nuisance=False`, and optionally provide your own nuisance regressors as a list (one numpy array per subject) as nuisance argument to GBRSA.fit(). In practice, we find that the result is much better with `auto_nuisance=True`.\n",
    "\n",
    "The idea of modeling the spatial noise correlation with the principal component decomposition of the residual noise is similar to that in GLMdenoise (http://kendrickkay.net/GLMdenoise/).\n",
    "Apparently one can imagine that the choice of the number of principal components used as nuisance regressors can influence the result. If you just choose 1 or 2, perhaps only the global drift would be captured. But including too many nuisance regressors would slow the fitting speed and might have risk of overfitting. Among all the algorithms we have tested with simulation data, th Gavish & Donoho algorithm appears the most robust and the estimate is closest to the true simulated number. But it does have a tendency to under-estimate the number of components, which is one limitation in (G)BRSA module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbrsa = GBRSA()\n",
    "# Initiate an instance\n",
    "\n",
    "gbrsa.fit(X=Y, design=[d.design_task for d in design],scan_onsets=scan_onsets)\n",
    "# The data to fit should be given to the argument X.\n",
    "# Design matrix goes to design. And so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can have a look at the estimated similarity in matrix gbrsa.C_. \n",
    "### We can also compare the ideal covariance above with the one recovered, gbrsa.U_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(gbrsa.C_, vmin=-0.1, vmax=1)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Estimated correlation structure\\n shared between voxels\\n'\n",
    "         'This constitutes the output of Bayesian RSA\\n')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(gbrsa.U_)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Estimated covariance structure\\n shared between voxels\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In contrast, we can have a look of the similarity matrix based on Pearson correlation between point estimates of betas of different conditions.\n",
    "#### This is what vanila RSA might give"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_point_corr = np.zeros((n_C, n_C))\n",
    "sum_point_cov = np.zeros((n_C, n_C))\n",
    "betas_point = [None] * n_subj\n",
    "for subj in range(n_subj):\n",
    "    regressor = np.insert(design[subj].design_task,\n",
    "                          0, 1, axis=1)\n",
    "    betas_point[subj] = np.linalg.lstsq(regressor, Y[subj])[0]\n",
    "    point_corr = np.corrcoef(betas_point[subj][1:, :])\n",
    "    point_cov = np.cov(betas_point[subj][1:, :])\n",
    "    sum_point_corr += point_corr\n",
    "    sum_point_cov += point_cov\n",
    "    if subj == 0:\n",
    "        fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "        plt.pcolor(point_corr, vmin=-0.1, vmax=1)\n",
    "        plt.xlim([0, 16])\n",
    "        plt.ylim([0, 16])\n",
    "        plt.colorbar()\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect(1)\n",
    "        plt.title('Correlation structure estimated\\n'\n",
    "                 'based on point estimates of betas\\n'\n",
    "                 'for subject {}'.format(subj))\n",
    "        plt.show()\n",
    "\n",
    "        fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "        plt.pcolor(point_cov)\n",
    "        plt.xlim([0, 16])\n",
    "        plt.ylim([0, 16])\n",
    "        plt.colorbar()\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect(1)\n",
    "        plt.title('Covariance structure of\\n'\n",
    "                 'point estimates of betas\\n'\n",
    "                 'for subject {}'.format(subj))\n",
    "        plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(sum_point_corr / n_subj, vmin=-0.1, vmax=1)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Correlation structure estimated\\n'\n",
    "         'based on point estimates of betas\\n'\n",
    "         'averaged over subjects')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(sum_point_cov / n_subj)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Covariance structure of\\n'\n",
    "         'point estimates of betas\\n'\n",
    "         'averaged over subjects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can make a comparison between the estimated SNR map and the true SNR map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = highest_subj\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "\n",
    "\n",
    "vmax = np.max([np.max(gbrsa.nSNR_[s]) for s in range(n_subj)])\n",
    "for s in range(n_subj):\n",
    "    im = axes[s].pcolor(np.reshape(gbrsa.nSNR_[s], [ROI_edge[s], ROI_edge[s]]),\n",
    "                        vmin=0,vmax=vmax)\n",
    "    axes[s].set_aspect(1)\n",
    "    \n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.75)\n",
    "\n",
    "plt.suptitle('estimated pseudo-SNR',fontsize=\"xx-large\" )\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "vmax = np.max([np.max(snr[s]) for s in range(n_subj)])\n",
    "for s in range(n_subj):\n",
    "    im = axes[s].pcolor(np.reshape(snr[s], [ROI_edge[s], ROI_edge[s]]),\n",
    "                        vmin=0,vmax=vmax)\n",
    "    axes[s].set_aspect(1)\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.75)\n",
    "plt.suptitle('simulated pseudo-SNR',fontsize=\"xx-large\" )\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMS_GBRSA = np.mean((gbrsa.C_ - ideal_corr)**2)**0.5\n",
    "RMS_RSA = np.mean((point_corr - ideal_corr)**2)**0.5\n",
    "print('RMS error of group Bayesian RSA: {}'.format(RMS_GBRSA))\n",
    "print('RMS error of standard RSA: {}'.format(RMS_RSA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also look at how SNRs are recovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "for s in range(n_subj):\n",
    "    im = axes[s].scatter(np.log(snr[s]) - np.mean(np.log(snr[s])),\n",
    "            np.log(gbrsa.nSNR_[s]))\n",
    "    if s == 0:\n",
    "        axes[s].set_ylabel('recovered log pseudo-SNR',fontsize='xx-large')\n",
    "    if s == int(n_subj/2):\n",
    "        axes[s].set_xlabel('true normalized log SNR',fontsize='xx-large')\n",
    "    axes[s].set_aspect(1)\n",
    "plt.suptitle('estimated vs. simulated normalized log SNR',fontsize=\"xx-large\" )\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "for s in range(n_subj):\n",
    "    im = axes[s].scatter(snr[s], gbrsa.nSNR_[s])\n",
    "    if s == 0:\n",
    "        axes[s].set_ylabel('recovered pseudo-SNR',fontsize='xx-large')\n",
    "    if s == int(n_subj/2):\n",
    "        axes[s].set_xlabel('true normalized SNR',fontsize='xx-large')\n",
    "    axes[s].set_aspect(1)\n",
    "plt.suptitle('estimated vs. simulated SNR',fontsize=\"xx-large\" )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also examine the relation between recovered betas and true betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "for s in range(n_subj):\n",
    "    im = axes[s].scatter(betas_simulated[s] , gbrsa.beta_[s])\n",
    "    if s == 0:\n",
    "        axes[s].set_ylabel('recovered betas by GBRSA',fontsize='xx-large')\n",
    "    if s == int(n_subj/2):\n",
    "        axes[s].set_xlabel('true betas',fontsize='xx-large')\n",
    "    axes[s].set_aspect(1)\n",
    "plt.suptitle('estimated vs. simulated betas, \\nby GBRSA',fontsize=\"xx-large\" )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "for s in range(n_subj):\n",
    "    im = axes[s].scatter(betas_simulated[s] , betas_point[s][1:, :])\n",
    "    if s == 0:\n",
    "        axes[s].set_ylabel('recovered betas by simple regression',fontsize='xx-large')\n",
    "    if s == int(n_subj/2):\n",
    "        axes[s].set_xlabel('true betas',fontsize='xx-large')\n",
    "    axes[s].set_aspect(1)\n",
    "plt.suptitle('estimated vs. simulated betas, \\nby simple regression',fontsize=\"xx-large\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Decoding\" from new data\n",
    "### Now we generate a new data set, assuming signal is the same but noise is regenerated. We want to use the transform() function of gbrsa to estimate the \"design matrix\" in this new dataset.\n",
    "We keep the signal the same as in training data, but generate new noise.\n",
    "Note that we did this purely for simplicity of simulation. It is totally fine and encouraged for the event timing to be different in your training and testing data. You just need to capture them in your design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_new = [None] * n_subj\n",
    "Y_new = [None] * n_subj\n",
    "for subj in range(n_subj):\n",
    "    \n",
    "    # generating noise\n",
    "    K_noise = noise_level[subj][:, np.newaxis] \\\n",
    "        * (np.exp(-dist2[subj] / noise_smooth_width**2 / 2.0) \\\n",
    "           + np.eye(n_V[subj]) * 0.1) * noise_level[subj]\n",
    "    # We make spatially correlated noise by generating\n",
    "    # noise at each time point from a Gaussian Process\n",
    "    # defined over the coordinates.\n",
    "    L_noise = np.linalg.cholesky(K_noise)\n",
    "    noise_new[subj] = np.zeros([n_T[subj], n_V[subj]])\n",
    "    noise_new[subj][0, :] = np.dot(L_noise, np.random.randn(n_V[subj]))\\\n",
    "        / np.sqrt(1 - rho1[subj]**2)\n",
    "    for i_t in range(1, n_T[subj]):\n",
    "        noise_new[subj][i_t, :] = noise_new[subj][i_t - 1, :] * rho1[subj] \\\n",
    "            + np.dot(L_noise,np.random.randn(n_V[subj]))\n",
    "    Y_new[subj] = signal[subj] + noise_new[subj] + inten[subj]\n",
    "\n",
    "\n",
    "\n",
    "ts, ts0 = gbrsa.transform(Y_new,scan_onsets=scan_onsets)\n",
    "# ts is the estimated task-related time course, with each column corresponding to the task condition of the same\n",
    "# column in design matrix.\n",
    "# ts0 is the estimated time courses that have the same spatial spread as those in the training data (X0).\n",
    "# It is possible some task related signal is still in X0 or ts0, but not captured by the design matrix.\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "for s in range(n_subj):\n",
    "    recovered_plot, = axes[s].plot(ts[s][:200, 8], 'b')\n",
    "    design_plot, = axes[s].plot(design[s].design_task[:200, 8], 'g')\n",
    "    if s == int(n_subj/2):\n",
    "        axes[s].set_xlabel('time',fontsize='xx-large')\n",
    "    \n",
    "fig.legend([design_plot, recovered_plot],\n",
    "           ['design matrix for one condition', 'recovered time course for the condition'],\n",
    "          fontsize='xx-large')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# We did not plot the whole time series for the purpose of seeing closely how much the two\n",
    "# time series overlap\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_subj, figsize=(25, 5))\n",
    "for s in range(n_subj):\n",
    "    c = np.corrcoef(design[s].design_task.T, ts[s].T)\n",
    "    im = axes[s].pcolor(c[0:16, 16:],vmin=-0.5,vmax=1)\n",
    "    axes[s].set_aspect(1)\n",
    "    if s == int(n_subj/2):\n",
    "        axes[s].set_xlabel('recovered time course',fontsize='xx-large')\n",
    "    if s == 0:\n",
    "        axes[s].set_ylabel('true design matrix',fontsize='xx-large')\n",
    "fig.suptitle('correlation between true design matrix \\nand the recovered task-related activity')\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.75)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('average SNR level:', snr_level)\n",
    "print('Apparently how much the recovered time course resembles the true design matrix depends on SNR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection by cross-validataion:\n",
    "Similar to BRSA, you can compare different models by cross-validating the parameters of one model learnt from some training data\n",
    "on some testing data. GBRSA provides a score() function, which returns you a pair of cross-validated log likelihood\n",
    "for testing data. The first returned item is a numpy array of the cross-validated log likelihood of the model you have specified, for the testing data of all the subjects.\n",
    "The second is a numpy arrary of those of a null model which assumes everything else the same except that there is no task-related activity.\n",
    "### Notice that comparing the score of your model of interest against its corresponding null model is not the only way to compare models. You might also want to compare against a model using the same set of design matrix, but a different rank (especially rank 1, which means all task conditions have the same response pattern, only differing in their magnitude).\n",
    "In general, in the context of GBRSA, a model means the timing of each event and the way these events are grouped, together with other trivial parameters such as the rank of the covariance matrix and the number of nuisance regressors. All these parameters can influence model performance.\n",
    "In future, we will provide interface to evaluate the predictive power for the data by different predefined similarity matrix or covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "width = 0.35\n",
    "[score, score_null] = gbrsa.score(X=Y_new, design=[d.design_task for d in design], scan_onsets=scan_onsets)\n",
    "\n",
    "plt.bar(np.arange(n_subj),np.asarray(score)-np.asarray(score_null), width=width)\n",
    "plt.ylim(np.min([np.asarray(score)-np.asarray(score_null)])-100, np.max([np.asarray(score)-np.asarray(score_null)])+100)\n",
    "plt.ylabel('cross-validated log likelihood')\n",
    "plt.xlabel('partipants')\n",
    "plt.title('Difference between cross-validated log likelihoods\\n of full model and null model\\non new data containing signal')\n",
    "plt.show()\n",
    "\n",
    "Y_nosignal = [noise_new[s] + inten[s] for s in range(n_subj)]\n",
    "[score_noise, score_null_noise] = gbrsa.score(X=Y_nosignal, design=[d.design_task for d in design], scan_onsets=scan_onsets)\n",
    "\n",
    "plt.bar(np.arange(n_subj),np.asarray(score_noise)-np.asarray(score_null_noise), width=width)\n",
    "plt.ylim(np.min([np.asarray(score_noise)-np.asarray(score_null_noise)])-100,\n",
    "         np.max([np.asarray(score_noise)-np.asarray(score_null_noise)])+100)\n",
    "plt.ylabel('cross-validated log likelihood')\n",
    "plt.xlabel('partipants')\n",
    "plt.title('Difference between cross-validated log likelihoods\\n of full model and null model\\non pure noise')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model performs better on testing data that has the same property of signal and noise with training data.\n",
    "#### Below, we fit the model to data containing only noise and test how it performs on data with signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrsa_noise = GBRSA()\n",
    "gbrsa_noise.fit(X=[noise[s] + inten[s] for s in range(n_subj)],\n",
    "                design=[d.design_task for d in design],scan_onsets=scan_onsets)\n",
    "Y_nosignal = [noise_new[s] + inten[s] for s in range(n_subj)]\n",
    "[score_noise, score_null_noise] = gbrsa_noise.score(X=Y_nosignal,\n",
    "                                                    design=[d.design_task for d in design], scan_onsets=scan_onsets)\n",
    "\n",
    "plt.bar(np.arange(n_subj),np.asarray(score_noise)-np.asarray(score_null_noise), width=width)\n",
    "plt.ylim(np.min([np.asarray(score_noise)-np.asarray(score_null_noise)])-100,\n",
    "         np.max([np.asarray(score_noise)-np.asarray(score_null_noise)])+100)\n",
    "plt.ylabel('cross-validated log likelihood')\n",
    "plt.xlabel('partipants')\n",
    "plt.title('Difference between cross-validated log likelihoods\\n of full model and null model\\ntrained on pure noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the difference is smaller but full model generally performs slightly worse, because of overfitting. This is expected.\n",
    "## So, after fitting a model to your data, you should also check cross-validated log likelihood on separate runs from the same group of participants, and make sure your model is at least better than a null model before you trust your similarity matrix. \n",
    "## Another diagnostic of bad model to your data is very small diagonal values in the shared covariance structure U_\n",
    "####  Shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(gbrsa_noise.U_)\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('covariance matrix of task conditions estimated from pure noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly, the memory demand might be high in case of big ROI, large number of participants, or fine grain in parameter space of log(SNR) and rho. If this happens, one can reduce parameters of `SNR_bins` (and  `logS_range` simultaneously if `SNR_prior` is set to `lognorm`, and consider running on clusters with bigger memory. But keep in mind reducing logS_range means we are putting a prior that the variance of SNR across voxels and participants is smaller."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
