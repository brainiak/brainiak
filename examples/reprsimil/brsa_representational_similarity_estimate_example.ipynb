{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This demo shows how to use the Bayesian Representational Similarity Analysis method in brainiak with a simulated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load some package which we will use in this demo.\n",
    "If you see error related to loading any package, you can install that package. For example, if you use Anaconda, you can use \"conda install matplotlib\" to install matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import scipy.spatial.distance as spdist\n",
    "import numpy as np\n",
    "from brainiak.reprsimil.brsa import BRSA\n",
    "import brainiak.utils.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You might want to keep a log of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    filename='brsa_example.log',\n",
    "    format='%(relativeCreated)6d %(threadName)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We want to simulate some data in which each voxel responds to different task conditions differently, but following a common covariance structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an example design matrix.\n",
    "The user should prepare their design matrix with their favorate software, such as using 3ddeconvolve of AFNI, or using SPM or FSL.\n",
    "The design matrix reflects your belief of how fMRI signal should respond to a task (if a voxel does respond).\n",
    "The common assumption is that a neural event that you are interested in will elicit a slow hemodynamic response in some voxels. The response peaks around 4-6 seconds after the event onset and dies down more than 12 seconds after the event. Therefore, typically you convolve a time series A, composed of delta (stem) functions reflecting the time of each neural event belonging to the same category (e.g. all trials in which a participant sees a face), with a hemodynamic response function B, to form the hypothetic response of any voxel to such type of neural event.\n",
    "For each type of event, such a convoluted time course can be generated. These time courses, put together, are called design matrix, reflecting what we believe a temporal signal would look like, if it exists in any voxel.\n",
    "Our goal is to figure out how the (spatial) response pattern of a population of voxels (in an Region of Interest, ROI) are similar or disimilar to different types of tasks (e.g., watching face vs. house, watching different categories of animals, different conditions of a cognitive task). So we need the design matrix in order to estimate the similarity matrix we are interested.\n",
    "\n",
    "We can use the utility called ReadDesign to read a design matrix generated from AFNI. For design matrix saved as Matlab data file by SPM or or other toolbox, you can use scipy.io.loadmat('YOURFILENAME') and extract the design matrix from the dictionary returned. Basically, the Bayesian RSA in this toolkit just needs a numpy array which is in size of {time points} * {condition}\n",
    "\n",
    "In typical fMRI analysis, some nuisance regressors such as head motion, baseline time series and slow drift are also entered into regression. In using our method, you should not include such regressors into the design matrix, because the spatial spread of such nuisance regressors might be quite different from the spatial spread of task related signal. Including such nuisance regressors in design matrix might influence the pseudo-SNR map, which in turn influence the estimation of the shared covariance matrix. \n",
    "\n",
    "### We concatenate the design matrix by 2 times, mimicking 2 runs of identical timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "design = utils.ReadDesign(fname=\"example_design.1D\")\n",
    "\n",
    "n_run = 2\n",
    "design.n_TR = design.n_TR * n_run\n",
    "design.design_task = np.tile(design.design_task[:,:-1],\n",
    "                             [n_run, 1])\n",
    "# The last \"condition\" in design matrix\n",
    "# codes for trials subjects made and error.\n",
    "# We ignore it here.\n",
    "\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 3),\n",
    "                 dpi=150, facecolor='w', edgecolor='k')\n",
    "plt.plot(design.design_task)\n",
    "plt.ylim([-0.2, 0.4])\n",
    "plt.title('hypothetic fMRI response time courses '\n",
    "          'of all conditions in addition to a DC component\\n'\n",
    "         '(design matrix)')\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "n_C = np.size(design.design_task, axis=1)\n",
    "# The total number of conditions.\n",
    "ROI_edge = 20\n",
    "# We simulate \"ROI\" of a square shape\n",
    "n_V = ROI_edge**2\n",
    "# The total number of simulated voxels\n",
    "n_T = design.n_TR\n",
    "# The total number of time points,\n",
    "# after concatenating all fMRI runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate data: noise + signal\n",
    "### First, we start with noise, which is Gaussian Process in space and AR(1) in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_bot = 0.5\n",
    "noise_top = 1.5\n",
    "noise_level = np.random.rand(n_V) * \\\n",
    "    (noise_top - noise_bot) + noise_bot\n",
    "# The standard deviation of the noise is in the range of [noise_bot, noise_top]\n",
    "# In fact, we simulate autocorrelated noise with AR(1) model. So the noise_level reflects\n",
    "# the independent additive noise at each time point (the \"fresh\" noise)\n",
    "\n",
    "# AR(1) coefficient\n",
    "rho1_top = 0.8\n",
    "rho1_bot = -0.2\n",
    "rho1 = np.random.rand(n_V) \\\n",
    "    * (rho1_top - rho1_bot) + rho1_bot\n",
    "\n",
    "noise_smooth_width = 10.0\n",
    "coords = np.mgrid[0:ROI_edge, 0:ROI_edge, 0:1]\n",
    "coords_flat = np.reshape(coords,[3, n_V]).T\n",
    "dist2 = spdist.squareform(spdist.pdist(coords_flat, 'sqeuclidean'))\n",
    "\n",
    "# generating noise\n",
    "K_noise = noise_level[:, np.newaxis] \\\n",
    "    * (np.exp(-dist2 / noise_smooth_width**2 / 2.0) \\\n",
    "       + np.eye(n_V) * 0.1) * noise_level\n",
    "# We make spatially correlated noise by generating\n",
    "# noise at each time point from a Gaussian Process\n",
    "# defined over the coordinates.\n",
    "plt.pcolor(K_noise)\n",
    "plt.colorbar()\n",
    "plt.xlim([0, ROI_edge * ROI_edge])\n",
    "plt.ylim([0, ROI_edge * ROI_edge])\n",
    "plt.title('Spatial covariance matrix of noise')\n",
    "plt.show()\n",
    "L_noise = np.linalg.cholesky(K_noise)\n",
    "noise = np.zeros([n_T, n_V])\n",
    "noise[0, :] = np.dot(L_noise, np.random.randn(n_V))\\\n",
    "    / np.sqrt(1 - rho1**2)\n",
    "for i_t in range(1, n_T):\n",
    "    noise[i_t, :] = noise[i_t - 1, :] * rho1 \\\n",
    "        + np.dot(L_noise,np.random.randn(n_V))\n",
    "# For each voxel, the noise follows AR(1) process:\n",
    "# fresh noise plus a dampened version of noise at\n",
    "# the previous time point.\n",
    "noise = noise + np.random.randn(n_V)\n",
    "fig = plt.figure(num=None, figsize=(12, 2), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "plt.plot(noise[:, 0])\n",
    "plt.title('noise in an example voxel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we simulate signals, assuming the magnitude of response to each condition follows a common covariance matrix. \n",
    "#### Our model allows to impose a Gaussian Process prior on the log(SNR) of each voxels. \n",
    "What this means is that SNR turn to be smooth and local, but betas (response amplitudes of each voxel to each condition) are not necessarily correlated in space. Intuitively, this is based on the assumption that voxels coding for related aspects of a task turn to be clustered (instead of isolated)\n",
    "\n",
    "Our Gaussian Process are defined on both the coordinate of a voxel and its mean intensity.\n",
    "This means that voxels close together AND have similar intensity should have similar SNR level. Therefore, voxels of white matter but adjacent to gray matters do not necessarily have high SNR level.\n",
    "\n",
    "If you have an ROI saved as a binary Nifti file, say, with name 'ROI.nii'\n",
    "Then you can use nibabel package to load the ROI and the following example code to retrive the coordinates of voxels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: the following code won't work if you just installed Brainiak and try this demo because ROI.nii does not exist. It just serves as an example for you to retrieve coordinates of voxels in an ROI. You can use the ROI_coords for the argument coords in BRSA.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import nibabel\n",
    "# ROI = nibabel.load('ROI.nii')\n",
    "# I,J,K = ROI.shape \n",
    "# all_coords = np.zeros((I, J, K, 3)) \n",
    "# all_coords[...,0] = np.arange(I)[:, np.newaxis, np.newaxis] \n",
    "# all_coords[...,1] = np.arange(J)[np.newaxis, :, np.newaxis] \n",
    "# all_coords[...,2] = np.arange(K)[np.newaxis, np.newaxis, :] \n",
    "# ROI_coords = nibabel.affines.apply_affine(\n",
    "#     ROI.affine, all_coords[ROI.get_data().astype(bool)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's keep in mind of the pattern of the ideal covariance / correlation below and see how well BRSA can recover their patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ideal covariance matrix\n",
    "ideal_cov = np.zeros([n_C, n_C])\n",
    "ideal_cov = np.eye(n_C) * 0.6\n",
    "ideal_cov[8:12, 8:12] = 0.8\n",
    "for cond in range(8, 12):\n",
    "    ideal_cov[cond,cond] = 1\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(ideal_cov)\n",
    "plt.colorbar()\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('ideal covariance matrix')\n",
    "plt.show()\n",
    "\n",
    "std_diag = np.diag(ideal_cov)**0.5\n",
    "ideal_corr = ideal_cov / std_diag / std_diag[:, None]\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(ideal_corr)\n",
    "plt.colorbar()\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('ideal correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following, pseudo-SNR is generated from a Gaussian Process defined on a \"square\" ROI, just for simplicity of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_full = np.linalg.cholesky(ideal_cov)        \n",
    "\n",
    "# generating signal\n",
    "snr_level = 0.6\n",
    "# Notice that accurately speaking this is not SNR.\n",
    "# The magnitude of signal depends not only on beta but also on x.\n",
    "# (noise_level*snr_level)**2 is the factor multiplied\n",
    "# with ideal_cov to form the covariance matrix from which\n",
    "# the response amplitudes (beta) of a voxel are drawn from.\n",
    "\n",
    "tau = 0.8\n",
    "# magnitude of Gaussian Process from which the log(SNR) is drawn\n",
    "smooth_width = 3.0\n",
    "# spatial length scale of the Gaussian Process, unit: voxel\n",
    "inten_kernel = 4.0\n",
    "# intensity length scale of the Gaussian Process\n",
    "# Slightly counter-intuitively, if this parameter is very large,\n",
    "# say, much larger than the range of intensities of the voxels,\n",
    "# then the smoothness has much small dependency on the intensity.\n",
    "\n",
    "\n",
    "inten = np.random.rand(n_V) * 20.0\n",
    "# For simplicity, we just assume that the intensity\n",
    "# of all voxels are uniform distributed between 0 and 20\n",
    "# parameters of Gaussian process to generate pseuso SNR\n",
    "# For curious user, you can also try the following commond\n",
    "# to see what an example snr map might look like if the intensity\n",
    "# grows linearly in one spatial direction\n",
    "\n",
    "# inten = coords_flat[:,0] * 2\n",
    "\n",
    "\n",
    "inten_tile = np.tile(inten, [n_V, 1])\n",
    "inten_diff2 = (inten_tile - inten_tile.T)**2\n",
    "\n",
    "K = np.exp(-dist2 / smooth_width**2 / 2.0 \n",
    "           - inten_diff2 / inten_kernel**2 / 2.0) * tau**2 \\\n",
    "    + np.eye(n_V) * tau**2 * 0.001\n",
    "# A tiny amount is added to the diagonal of\n",
    "# the GP covariance matrix to make sure it can be inverted\n",
    "L = np.linalg.cholesky(K)\n",
    "snr = np.exp(np.dot(L, np.random.randn(n_V))) * snr_level\n",
    "sqrt_v = noise_level * snr\n",
    "betas_simulated = np.dot(L_full, np.random.randn(n_C, n_V)) * sqrt_v\n",
    "signal = np.dot(design.design_task, betas_simulated)\n",
    "\n",
    "\n",
    "Y = signal + noise \n",
    "# The data to be fed to the program.\n",
    "\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(np.reshape(snr, [ROI_edge, ROI_edge]))\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('pseudo-SNR in a square \"ROI\"')\n",
    "plt.show()\n",
    "\n",
    "idx = np.argmin(np.abs(snr - np.median(snr)))\n",
    "# choose a voxel of medium level SNR.\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "noise_plot, = plt.plot(noise[:,idx],'g')\n",
    "signal_plot, = plt.plot(signal[:,idx],'r')\n",
    "plt.legend([noise_plot, signal_plot], ['noise', 'signal'])\n",
    "plt.title('simulated data in an example voxel'\n",
    "          ' with pseudo-SNR of {}'.format(snr[idx]))\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "data_plot, = plt.plot(Y[:,idx],'b')\n",
    "plt.legend([data_plot], ['observed data of the voxel'])\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "idx = np.argmin(np.abs(snr - np.max(snr)))\n",
    "# display the voxel of the highest level SNR.\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "noise_plot, = plt.plot(noise[:,idx],'g')\n",
    "signal_plot, = plt.plot(signal[:,idx],'r')\n",
    "plt.legend([noise_plot, signal_plot], ['noise', 'signal'])\n",
    "plt.title('simulated data in the voxel with the highest'\n",
    "          ' pseudo-SNR of {}'.format(snr[idx]))\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(12, 4), dpi=150,\n",
    "                 facecolor='w', edgecolor='k')\n",
    "data_plot, = plt.plot(Y[:,idx],'b')\n",
    "plt.legend([data_plot], ['observed data of the voxel'])\n",
    "plt.xlabel('time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reason that the pseudo-SNRs in the example voxels are not too small, while the signal looks much smaller is because we happen to have low amplitudes in our design matrix. The true SNR depends on both the amplitudes in design matrix and the pseudo-SNR. Therefore, be aware that pseudo-SNR does not directly reflects how much signal the data have, but rather a map indicating the relative strength of signal in differerent voxels.\n",
    "#### When you have multiple runs, the noise won't be correlated between runs. Therefore, you should tell BRSA when is the onset of each scan. \n",
    "#### Note that the data (variable Y above) you feed to BRSA is the concatenation of data from all runs along the time dimension, as a 2-D matrix of time x space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scan_onsets = np.int32(np.linspace(0, design.n_TR,num=n_run + 1)[: -1])\n",
    "print('scan onsets: {}'.format(scan_onsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Bayesian RSA to our simulated data\n",
    "### The data should be z-scored along time dimension. Otherwise voxels of high variance in noise can dominate the estimation of spatially shared noise component.\n",
    "\n",
    "The nuisance regressors in typical fMRI analysis (such as head motion signal) are replaced by principal components estimated from residuals after subtracting task-related response. n_nureg tells the model how many principal components to keep from the residual as nuisance regressors, in order to account for spatial correlation in noise. \n",
    "If you prefer not using this approach based on principal components of residuals, you can set auto_nuisance=False, and optionally provide your own nuisance regressors as nuisance argument to BRSA.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_z = scipy.stats.zscore(Y,axis=0)\n",
    "Y_std = np.std(Y,axis=0)\n",
    "# Z-scoreing the data\n",
    "\n",
    "brsa = BRSA(GP_space=True, GP_inten=True,\n",
    "            n_nureg=10)\n",
    "# Initiate an instance, telling it\n",
    "# that we want to impose Gaussian Process prior\n",
    "# over both space and intensity.\n",
    "\n",
    "\n",
    "brsa.fit(X=Y_z, design=design.design_task,\n",
    "         coords=coords_flat, inten=inten, scan_onsets=scan_onsets)\n",
    "# The data to fit should be given to the argument X.\n",
    "# Design matrix goes to design. And so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can have a look at the estimated similarity in matrix brsa.C_. \n",
    "#### We can also compare the ideal covariance above with the one recovered, brsa.U_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(brsa.C_, vmin=-0.1, vmax=1)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Estimated correlation structure\\n shared between voxels\\n'\n",
    "         'This constitutes the output of Bayesian RSA\\n')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(brsa.U_)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Estimated covariance structure\\n shared between voxels\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In contrast, we can have a look of the similarity matrix based on Pearson correlation between point estimates of betas of different conditions.\n",
    "#### This is what vanila RSA might give"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regressor = np.insert(design.design_task,\n",
    "                      0, 1, axis=1)\n",
    "betas_point = np.linalg.lstsq(regressor, Y_z)[0]\n",
    "point_corr = np.corrcoef(betas_point[1:, :])\n",
    "point_cov = np.cov(betas_point[1:, :])\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(point_corr, vmin=-0.1, vmax=1)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Correlation structure estimated\\n'\n",
    "         'based on point estimates of betas\\n')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(4, 4), dpi=100)\n",
    "plt.pcolor(point_cov)\n",
    "plt.xlim([0, 16])\n",
    "plt.ylim([0, 16])\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.title('Covariance structure of\\n'\n",
    "         'point estimates of betas\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can make a comparison between the estimated SNR map and the true SNR map (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(5, 5), dpi=100)\n",
    "plt.pcolor(np.reshape(brsa.nSNR_, [ROI_edge, ROI_edge]), vmin=0, vmax=5)\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "ax.set_title('estimated pseudo-SNR')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(5, 5), dpi=100)\n",
    "plt.pcolor(np.reshape(snr / np.exp(np.mean(np.log(snr))),\n",
    "                      [ROI_edge, ROI_edge]), vmin=0, vmax=5)\n",
    "plt.colorbar()\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "ax.set_title('true normalized pseudo-SNR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RMS_BRSA = np.mean((brsa.C_ - ideal_corr)**2)**0.5\n",
    "RMS_RSA = np.mean((point_corr - ideal_corr)**2)**0.5\n",
    "print('RMS error of Bayesian RSA: {}'.format(RMS_BRSA))\n",
    "print('RMS error of standard RSA: {}'.format(RMS_RSA))\n",
    "print('Recovered spatial smoothness length scale: '\n",
    "      '{}, vs. true value: {}'.format(brsa.lGPspace_, smooth_width))\n",
    "print('Recovered intensity smoothness length scale: '\n",
    "      '{}, vs. true value: {}'.format(brsa.lGPinten_, inten_kernel))\n",
    "print('Recovered standard deviation of GP prior: '\n",
    "      '{}, vs. true value: {}'.format(brsa.bGP_, tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirically, the smoothness turns to be over-estimated when signal is weak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also look at how other parameters are recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(rho1, brsa.rho_)\n",
    "plt.xlabel('true AR(1) coefficients')\n",
    "plt.ylabel('recovered AR(1) coefficients')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(np.log(snr) - np.mean(np.log(snr)),\n",
    "            np.log(brsa.nSNR_))\n",
    "plt.xlabel('true normalized log SNR')\n",
    "plt.ylabel('recovered log pseudo-SNR')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even though the variation reduced in estimated pseudo-SNR (due to overestimation of smoothness of the GP prior under low SNR situation), betas recovered by the model has higher correlation with true betas than doing simple regression, shown below. Obiously there is shrinkage of the estimated betas, as a result of variance-bias tradeoff. But we think such shrinkage does preserve the patterns of betas, and therefore the result is suitable to be further used for decoding purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(betas_simulated / Y_std, brsa.beta_)\n",
    "plt.xlabel('true betas (response amplitudes)')\n",
    "plt.ylabel('recovered betas by Bayesian RSA')\n",
    "ax = plt.gca()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(betas_simulated / Y_std, betas_point[1:, :])\n",
    "plt.xlabel('true betas (response amplitudes)')\n",
    "plt.ylabel('recovered betas by simple regression')\n",
    "ax = plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The singular decomposition of noise, and the comparison between the first principal component of noise and the first principal component returned by the model.\n",
    "Apparently one can imagine that the choice of the number of principal components used as nuisance regressors can influence the result. If you just choose 1 or 2, perhaps only the baseline and drift would be captured. But including too many nuisance regressors would slow the fitting speed and might have risk of overfitting. The users might consider starting in the range of 5-20. In future, we may consider using cross validation to determine the number of nuisance regressors, similar as in GLMdenoise (http://kendrickkay.net/GLMdenoise/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(Y_z - signal / Y_std)\n",
    "plt.plot(s)\n",
    "plt.xlabel('principal component')\n",
    "plt.ylabel('singular value of simulated noise')\n",
    "plt.show()\n",
    "\n",
    "plt.pcolor(np.reshape(v[0,:], [ROI_edge, ROI_edge]))\n",
    "plt.title('Weights of the first principal component in noise')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.pcolor(np.reshape(brsa.beta0_[0,:], [ROI_edge, ROI_edge]))\n",
    "plt.title('Weights of the first recovered principal component in noise')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
