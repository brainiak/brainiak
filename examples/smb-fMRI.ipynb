{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMBFMRI \n",
    "(Simultaneous model-based fMRI)\n",
    "\n",
    "This is inspired, with some small modifications, by the work int hese two papers: \n",
    "\n",
    "Turner, B. M., Forstmann, B. U., Wagenmakers, E.-J., Brown, S. D., Sederberg, P. B., & Steyvers, M. (2013). A Bayesian framework for simultaneously modeling neural and behavioral data. NeuroImage, 72, 193–206. doi:10.1016/j.neuroimage.2013.01.048\n",
    "Turner, B. M., van Maanen, L., & Forstmann, B. U. (2015). Informing cognitive abstractions through neuroimaging: The neural drift diffusion model. Psychological Review, 122(2), 312–336. doi:10.1037/a0038894\n",
    "\n",
    "In traditional model-based fMRI (e.g. O’Doherty, J. P., Hampton, A., & Kim, H. (2007) doi:10.1196/annals.1390.022), one estimates cognitive model parameters from behavioral data, and then attempts to estimate neural correlates of these model parameters from fMRI data. The use of point estimates of cognitive model parameters overestimates confidence in the combined model, while worsening the estimate. A particularly notable deficiency is that model-based fMRI makes it impossible to estimate trial-by-trial changes of cogitive model parameters without an explicit trial-by-trial learning model. \n",
    "\n",
    "A better approach is to simultaneously estimate the cognitive model parameters and their neural correlates. We do so in a Bayesian setting. We specify 4 things: a dimension reduction model for brain data, a neural model for the trial-by-trial distribution of neural data, a cognitive model for behavioral data, and a model of their connection. The cognitive model serves as a \"dimension augmentation\" model for behavioral data, inferring some latent properties from the distribution of behavioral responses. \n",
    "\n",
    "In the Turner et al. papers, the dimension reduction is done independently, so they use point estimates of the features rather than distirbutions. Conveniently this means we can directly reuse feature extraction pipelines like TFA for doing this off-the-shelf. That said, the right way to do this would be in a semi-supervised way where the dimension reduction is informed by the rest of the model. \n",
    "\n",
    "We apply this here to the NYU slow event-related flanker task, which allows us to use wiener first passage time distributions to model behavioral response times and choices. The dataset is available at https://openfmri.org/dataset/ds000102/, though ideally for this example we would find a dataset in the examples provided with some existing package (but is has to be a simple behavioral task like random dots, stroop/flanker, etc). \n",
    "\n",
    "Here is our model, for a single subject only: \n",
    "\n",
    "$$\n",
    "y_{i} \\sim WFPT(\\psi)\\\\\n",
    "b_{i} \\sim \\mathcal{N}(\\phi, S)\\\\\n",
    "[\\psi, b] \\sim \\mathcal{N}(\\mu, \\Sigma)\n",
    "$$\n",
    "\n",
    "Here $i$ indexes behavioral trials (and TRs during which they occur -- since this is an event-related design, the mapping is trivial). $y$ is a tuple of RT and choice, drawn from the wiener first passage time (\"DDM\") distribution with DDM (cognitive) parameters $\\psi$. $b_i$ is some dimension-reduced representation of the fMRI data for the $i$th TR (in our case, the output from TFA, though it could also be ICA/PCA). In a perfect world, we would explicitly model the conditional distribution of $b$ and the raw neural data here as well. \n",
    "\n",
    "To do this, first we need to load data and do TFA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from brainiak.factor_analysis.tfa import TFA\n",
    "from wfpt.wfpt import wfpt_logp\n",
    "from scipy.stats import multivariate_normal\n",
    "import logging\n",
    "import sys\n",
    "import scipy.optimize as op\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# modified from tfa extract example\n",
    "def extract_data(nifti_file, mask_file=None, zscore=True, detrend=True, smoothing_fwmw=False):\n",
    "    if mask_file is None:\n",
    "        #whole brain, get coordinate info from nifti_file itself\n",
    "        mask = nib.load(nifti_file)\n",
    "    else:\n",
    "        mask = nib.load(mask_file)\n",
    "    affine = mask.get_affine()\n",
    "    if mask_file is None:\n",
    "        mask_data = mask.get_data()\n",
    "        if mask_data.ndim == 4:\n",
    "            #get mask in 3D\n",
    "            img_data_type = mask.header.get_data_dtype()\n",
    "            n_tr = mask_data.shape[3]\n",
    "            mask_data = mask_data[:,:,:,n_tr//2].astype(bool)\n",
    "            mask = nib.Nifti1Image(mask_data.astype(img_data_type), affine)\n",
    "        else:\n",
    "            mask_data = mask_data.astype(bool)\n",
    "    else:\n",
    "        mask_data = mask.get_data().astype(bool)\n",
    "\n",
    "    #get voxel coordinates\n",
    "    R = np.float64(np.argwhere(mask_data))\n",
    "\n",
    "    #get scanner RAS coordinates based on voxel coordinates\n",
    "    if affine is not []:\n",
    "        R = (np.dot(affine[:3,:3], R.T) + affine[:3,3:4]).T\n",
    "\n",
    "    #get ROI data, and run preprocessing\n",
    "    nifti_masker = NiftiMasker(mask_img=mask, standardize=zscore, detrend=detrend, smoothing_fwhm=smoothing_fwmw)\n",
    "    img = nib.load(nifti_file)\n",
    "    all_images = np.float64(nifti_masker.fit_transform(img))\n",
    "    data = all_images.T.copy()\n",
    "\n",
    "    #save data\n",
    "    return data, R\n",
    "\n",
    "\n",
    "# get brain data\n",
    "neurodata1,R1 = extract_data('PATH_TO_DOWNLOADED_DATA.nii.gz')\n",
    "\n",
    "# to make TFA quick for testing, only do it on the TRs where we have behavior. To do this, first load behavior: \n",
    "behavdata = pd.read_table('PATH_TO_DOWNLOADED_DATA.tsv')\n",
    "\n",
    "# figure out which neurodata had behavior in it\n",
    "behavdataTRs = (behavdata.onset/2).values.astype(int)\n",
    "# subset neuro data\n",
    "neurodata = neurodata[:,behavdataTRs]\n",
    "\n",
    "# Run TFA\n",
    "n_voxel, n_tr = neurodata.shape\n",
    "tfa = TFA(K=5, max_num_voxel=int(n_voxel*0.5), max_num_tr=int(n_tr*0.5), verbose=True)\n",
    "tfa.fit(neurodata, R)\n",
    "# get weights (and transpose to have standard format)\n",
    "brainWeights = tfa.W_.T\n",
    "\n",
    "# extract behavior data\n",
    "behavdata = np.array([behavdata.response_time.values,(behavdata.correctness=='correct').values.astype(int), behavdata.StimVar.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDM parameters have finite support. Infinite support makes it much easier to sample and optimize, so we write some transformations here (maybe these should live in wfpt_py?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a vec'd likelihood\n",
    "wfpt_logp_vec = np.vectorize(wfpt_logp)\n",
    "\n",
    "def wfptParamTransform(x0, t0, cong_a, incong_a, cong_z, incong_z):\n",
    "    # go from infinite support to where we actually want to be\n",
    "    t0 = np.exp(t0)\n",
    "    cong_a = np.exp(cong_a)\n",
    "    incong_a = np.exp(incong_a)\n",
    "    cong_z = np.exp(cong_z)\n",
    "    incong_z = np.exp(incong_z)\n",
    "    return x0, t0, cong_a, incong_a, cong_z, incong_z\n",
    "\n",
    "def wfptParamUntransform(x0, t0, cong_a, incong_a, cong_z, incong_z):\n",
    "    # go from infinite support to where we actually want to be\n",
    "    t0 = np.log(t0)\n",
    "    cong_a = np.log(cong_a)\n",
    "    incong_a = np.log(incong_a)\n",
    "    cong_z = np.log(cong_z)\n",
    "    incong_z = np.log(incong_z)\n",
    "    return x0, t0, cong_a, incong_a, cong_z, incong_z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelLik(p, neurodata, behavdata):\n",
    "    # unpack parameters\n",
    "    x0, t0, cong_a, incong_a, cong_z, incong_z, *neuroparams = p\n",
    "    # transform them into positive support as needed\n",
    "    x0, t0, cong_a, incong_a, cong_z, incong_z = wfptParamTransform(x0, t0, cong_a, incong_a, cong_z, incong_z)\n",
    "\n",
    "    logger.debug(\"behav params: x0=%f, t0=%f, cong_a=%f, incong_a=%f, cong_z=%f, incong_z=%f\" % (x0, t0, cong_a, incong_a, cong_z, incong_z))\n",
    "\n",
    "    # 5 TFA centers so neuroparams will have a length-5 vector\n",
    "    neuromean = np.array(neuroparams[0:5])\n",
    "    # next, cholesky of 5x5 covariance matrix (to ensure positive semidefinite)\n",
    "    neuroChol = np.zeros((5, 5))\n",
    "    neuroChol[np.triu_indices(5, 0)] = np.array(neuroparams[5:20])\n",
    "    neuroCov = neuroChol.T @ neuroChol\n",
    "\n",
    "    logger.debug(\"neuroDet %f\" % np.linalg.det(neuroCov) )\n",
    "    # if det is 0 we have singular covariance, return -inf? ideally we would never get here\n",
    "    if np.linalg.det(neuroCov) <= 1e-5:\n",
    "        logger.debug(\"singular neurocov!\")\n",
    "        logger.debug(neuroChol)\n",
    "        logger.debug(neuroCov)\n",
    "        return -np.inf\n",
    "\n",
    "    # next the hyperprior will have a 11-place mean\n",
    "    hypermean = np.array(neuroparams[20:31])\n",
    "    # hypercov is the rest\n",
    "    hyperChol = np.zeros((11,11))\n",
    "    hyperChol[np.triu_indices(11, 0)] = np.array(neuroparams[31:])\n",
    "    hyperCov = hyperChol.T @ hyperChol\n",
    "    logger.debug(\"hyperDet %f\" % np.linalg.det(hyperCov) )\n",
    "\n",
    "    # again if det is 0 return -inf\n",
    "    if np.linalg.det(hyperCov) <= 1e-5:\n",
    "        logger.debug(\"singular hypercov!\")\n",
    "        logger.debug(hyperChol)\n",
    "        logger.debug(hyperCov)\n",
    "        return -np.inf\n",
    "\n",
    "    # unpack behav data\n",
    "    rts, accs, conds = behavdata\n",
    "    \n",
    "    # dispense paraams according to condition\n",
    "    a = np.where(conds==1, cong_a, incong_a)\n",
    "    z = np.where(conds==1, cong_z, incong_z)\n",
    "    \n",
    "    # ddm likelihood\n",
    "    ddmlik = np.sum(wfpt_logp_vec(rts, accs, x0, t0, a, z, eps = 1e-10))\n",
    "    \n",
    "    # if ddm likelihood is 0 might as well return here\n",
    "    if not np.isfinite(ddmlik):\n",
    "        return -np.inf\n",
    "\n",
    "    # neuro likelihood\n",
    "    neurolik = np.sum(multivariate_normal.logpdf(neurodata, neuromean, neuroCov))\n",
    "\n",
    "    # hyper likelihood\n",
    "    hyperlik = multivariate_normal.logpdf(np.r_[x0, t0, cong_a, incong_a, cong_z, incong_z, neuromean], hypermean, hyperCov)\n",
    "\n",
    "    logger.debug(\"ddmlik %f, neurolik %f, hyperlik %f\" % (ddmlik, neurolik, hyperlik))\n",
    "\n",
    "    return ddmlik + neurolik + hyperlik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emcee recommends finding the MLE to initialize the samplers, so we try to do this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -modelLik(*args)\n",
    "\n",
    "# generate initial guess\n",
    "psi0 = np.array([0.1, 0.3, 1, 0.9, 1.5, 1])\n",
    "neuromean0 = np.zeros(5)\n",
    "neurocov0 = np.eye(5)[np.triu_indices(5, 0)].flatten()\n",
    "hypermean0 = np.zeros(11)\n",
    "hypercov0 = np.eye(11)[np.triu_indices(11, 0)].flatten()\n",
    "\n",
    "x0 = np.r_[wfptParamUntransform(*psi0), neuromean0, neurocov0, hypermean0, hypercov0]\n",
    "\n",
    "# test initial guess\n",
    "modelLik(x0, brainWeights, behavdata)\n",
    "\n",
    "# find MLE\n",
    "result = op.minimize(nll, x0, args=(brainWeights, behavdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do MCMC! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "ndim, nwalkers = x0.shape[0], 350\n",
    "# initial condition is MLE plus gaussian ball\n",
    "pos = [result['x'] + 0.1*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "\n",
    "# create sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, modelLik, args=(brainWeights, behavdata))\n",
    "\n",
    "sampler.run_mcmc(pos, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
