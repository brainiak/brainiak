{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDMS time segment matching example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the MDMS model on some datasets, and test the model on a completely unseen dataset to assess the quality of transfer learning between fMRI datasets using MDMS. As a toy example, we only use three datasets here. You can use more datasets in your experiment to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single node example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import stats\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "import pickle as pkl\n",
    "from brainiak.funcalign.mdms import MDMS, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 75 # number of features, k\n",
    "n_iter = 30 # number of iterations of EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation of MDMS and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(MDMS)\n",
    "help(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/multi_dataset.pickle','rb') as f:\n",
    "    data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two dataset structure options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be organized in two ways:\n",
    "\n",
    "1) A dict of list of 2D arrays, where data[d] is a list of data in dataset d and d is the name of the dataset.\n",
    "Element i in the list has shape=[voxels_i, samples_d], which is the fMRI data of the i'th subject in d.\n",
    "\n",
    "- If datasets are in this format, you still need a JSON file (or JSON files) with datasets information.  \n",
    "Each JSON file should contain a dict or a list of dict where each dict has information of one dataset. \n",
    "Each dict must have 'dataset', 'num_of_subj', and 'subjects' where 'dataset' is the name of the dataset, \n",
    "'num_of_subj' is the number of subjects in the dataset, and 'subjects' is a list of strings with names \n",
    "of subjects in the dataset in the same order as in the dataset. \n",
    "\n",
    "- Example of a JSON file: \n",
    "    [{'dataset':'MyData','num_of_subj':3,'subjects':['Adam','Bob','Carol']}, \n",
    "    {'dataset':'MyData2','num_of_subj':2,'subjects':['Tom','Bob']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our loaded datasets are in this format, let's inspect its structure\n",
    "print ('------ Datasets in data (dict keys) ------')\n",
    "print (list(data.keys()))\n",
    "print ('------ Type and length of the sherlock dataset (number of subjects) ------')\n",
    "print (type(data['sherlock']))\n",
    "print (len(data['sherlock']))\n",
    "print ('------ Shape of one subject in the sherlock dataset (voxel x sample) ------')\n",
    "print (data['sherlock'][0].shape)\n",
    "print ('The datasets are masked to regions of interest (ROI) : Default Mode Network (DMN)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corresponding JSON file\n",
    "! cat data/multi_dataset.json\n",
    "\n",
    "# convert the JSON file to a Dataset object\n",
    "ds_struct = Dataset('data/multi_dataset.json')\n",
    "\n",
    "# display information of datasets\n",
    "print ('------ Number of datasets ------')\n",
    "print (ds_struct.num_dataset)\n",
    "print ('------ Number of subjects ------')\n",
    "print (ds_struct.num_subj)\n",
    "print ('------ Visualize connectivity between datasets: datasets as nodes, number of shared subjects as edges------')\n",
    "ds_struct.visualize_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) When it is a dict of dict of 2D arrays, where data[d][s] has shape=[voxels_s, samples_d], which is the fMRI \n",
    "data of subject s in dataset d, where s is the name of the subject and d is the name of the dataset.\n",
    "\n",
    "- If a dataset is in this format, you don't need the JSON file to define datasets structure. MDMS can infer the \n",
    "structure from data.\n",
    "\n",
    "- Example: data['sherlock'] = {'s1': matrix1, 's4': matrix2}, where matrix1 and matrix2 are voxels x samples matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate training and testing data and zscore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use ['greeneye', 'sherlock'] to train and 'milky' to test. Note that we only test on subjects that are\n",
    "# in at least one training dataset as well.\n",
    "\n",
    "# get info of test data \n",
    "test_ds = 'milky'\n",
    "test_subj_list = ds_struct.subj_in_dataset[test_ds]\n",
    "test_data = data[test_ds]\n",
    "\n",
    "# remove test dataset from the dataset structure without changing the data and MDMS will handle it automatically\n",
    "_ = ds_struct.remove_dataset([test_ds])\n",
    "\n",
    "# remove subjects in test_ds that are not in any training dataset\n",
    "train_subj = set(ds_struct.get_subjects_list()) # all subjects in training set\n",
    "test_subj_idx_to_keep = [] # index of subjects to keep\n",
    "for idx, subj in enumerate(test_subj_list):\n",
    "    if subj in train_subj:\n",
    "        test_subj_idx_to_keep.append(idx)\n",
    "test_subj_list = [test_subj_list[idx] for idx in test_subj_idx_to_keep]\n",
    "test_data = [test_data[idx] for idx in test_subj_idx_to_keep]\n",
    "\n",
    "# compute voxels mean and std of each subject from training data and use them to standardize training and testing data\n",
    "mean, std = {}, {} # mean and std of each subject\n",
    "matrix_csr = ds_struct.matrix.tocsr(copy=True)\n",
    "for subj in range(ds_struct.num_subj): # iterate through all subjects\n",
    "    subj_name = ds_struct.idx_to_subject[subj]\n",
    "    indices = matrix_csr[subj,:].indices # indices of datasets with this subject\n",
    "    # aggregate all data from this subject\n",
    "    for idx, ds_idx in enumerate(indices):\n",
    "        if idx == 0:\n",
    "            mtx_tmp = data[ds_struct.idx_to_dataset[ds_idx]][ds_struct.dok_matrix[subj,ds_idx]-1]\n",
    "        else:\n",
    "            mtx_tmp = np.concatenate((mtx_tmp, data[ds_struct.idx_to_dataset[ds_idx]][ds_struct.dok_matrix[subj,ds_idx]-1]),axis=1)\n",
    "    # compute mean and std\n",
    "    mean[subj_name] = np.mean(mtx_tmp, axis=1)\n",
    "    std[subj_name] = np.std(mtx_tmp, axis=1)\n",
    "    # standardize training data\n",
    "    for ds_idx in indices:\n",
    "        ds_name, idx_in_ds = ds_struct.idx_to_dataset[ds_idx], ds_struct.dok_matrix[subj,ds_idx]-1\n",
    "        data[ds_name][idx_in_ds] = np.nan_to_num((data[ds_name][idx_in_ds]-mean[subj_name][:,None])/std[subj_name][:,None])\n",
    "        \n",
    "# use the mean and std computed from training data to standardize testing data\n",
    "for idx, subj in enumerate(test_subj_list):\n",
    "    test_data[idx] = np.nan_to_num((test_data[idx]-mean[subj][:,None])/std[subj][:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit MDMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MDMS(features=features, n_iter=n_iter)\n",
    "\n",
    "# Two ways to fit the model based on two dataset structure options\n",
    "# 1) When data is a dict of list of 2D arrays (as in our case), you need to have the ds_struct built from JSON files. \n",
    "# But you have the flexibility to keep testing data in 'data' as well, and MDMS will only train on data in ds_struct.\n",
    "model.fit(data, ds_struct)\n",
    "\n",
    "# 2) When data is a dict of dict of 2D arrays, you don't need the ds_struct, but you need to remove all data not meant \n",
    "# to be used during the training phase.\n",
    "# model.fit(data)  # uncomment this line if you have this kind of dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Segment Matching Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This experiment is an easy sanity check of the quality of fitting. The higher the accuracy, the better.\n",
    "def time_segment_matching(data, win_size=6): \n",
    "    nsubjs = len(data)\n",
    "    (ndim, nsample) = data[0].shape\n",
    "    accu = np.zeros(shape=nsubjs)\n",
    "    nseg = nsample - win_size \n",
    "    # mysseg prediction prediction\n",
    "    trn_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "    # the trn data also include the tst data, but will be subtracted when \n",
    "    # calculating A\n",
    "    for m in range(nsubjs):\n",
    "        for w in range(win_size):\n",
    "            trn_data[w*ndim:(w+1)*ndim,:] += data[m][:,w:(w+nseg)]\n",
    "    for tst_subj in range(nsubjs):\n",
    "        tst_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "        for w in range(win_size):\n",
    "            tst_data[w*ndim:(w+1)*ndim,:] = data[tst_subj][:,w:(w+nseg)]\n",
    "\n",
    "        A =  np.nan_to_num(stats.zscore((trn_data - tst_data),axis=0, ddof=1))\n",
    "        B =  np.nan_to_num(stats.zscore(tst_data,axis=0, ddof=1))\n",
    "\n",
    "        # compute correlation matrix\n",
    "        corr_mtx = compute_correlation(B.T,A.T)\n",
    "    \n",
    "        for i in range(nseg):\n",
    "            for j in range(nseg):\n",
    "                if abs(i-j)<win_size and i != j :\n",
    "                    corr_mtx[i,j] = -np.inf\n",
    "        max_idx =  np.argmax(corr_mtx, axis=1)\n",
    "        accu[tst_subj] = sum(max_idx == range(nseg)) / float(nseg)\n",
    "\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "transformed = model.transform(test_data, test_subj_list) # test_subj_list: element i is the name of subject of X[i]\n",
    "\n",
    "# zscore the transformed data\n",
    "for subj in range(len(transformed)):\n",
    "    transformed[subj] = stats.zscore(transformed[subj], axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = time_segment_matching(transformed)\n",
    "accu_mean = np.mean(accu)\n",
    "accu_se = stats.sem(accu)\n",
    "print ('Accuracy is {} +- {}'.format(accu_mean, accu_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the result with single dataset case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train MDMS using one dataset 'greeneye' and test on 'milky' to show the extra training dataset ('sherlock') indeed transfer useful information. Note that the number of testing subjects is the same as the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and JSON file again\n",
    "with open('data/multi_dataset.pickle','rb') as f:\n",
    "    data = pkl.load(f)\n",
    "ds_struct = Dataset('data/multi_dataset.json')    \n",
    "    \n",
    "# get info of test data \n",
    "test_ds = 'milky'\n",
    "test_subj_list = ds_struct.subj_in_dataset[test_ds]\n",
    "test_data = data[test_ds]\n",
    "\n",
    "# remove test dataset from the dataset structure\n",
    "_ = ds_struct.remove_dataset([test_ds])\n",
    "\n",
    "# remove the other training datasets\n",
    "_ = ds_struct.remove_dataset(['sherlock'])\n",
    "\n",
    "# remove subjects in test_ds that are not in any training dataset\n",
    "train_subj = set(ds_struct.get_subjects_list()) # all subjects in training set\n",
    "test_subj_idx_to_keep = [] # index of subjects to keep\n",
    "for idx, subj in enumerate(test_subj_list):\n",
    "    if subj in train_subj:\n",
    "        test_subj_idx_to_keep.append(idx)\n",
    "test_subj_list = [test_subj_list[idx] for idx in test_subj_idx_to_keep]\n",
    "test_data = [test_data[idx] for idx in test_subj_idx_to_keep]\n",
    "\n",
    "# compute voxels mean and std of each subject from training data and use them to standardize training and testing data\n",
    "mean, std = {}, {} # mean and std of each subject\n",
    "matrix_csr = ds_struct.matrix.tocsr(copy=True)\n",
    "for subj in range(ds_struct.num_subj): # iterate through all subjects\n",
    "    subj_name = ds_struct.idx_to_subject[subj]\n",
    "    indices = matrix_csr[subj,:].indices # indices of datasets with this subject\n",
    "    # aggregate all data from this subject\n",
    "    for idx, ds_idx in enumerate(indices):\n",
    "        if idx == 0:\n",
    "            mtx_tmp = data[ds_struct.idx_to_dataset[ds_idx]][ds_struct.dok_matrix[subj,ds_idx]-1]\n",
    "        else:\n",
    "            mtx_tmp = np.concatenate((mtx_tmp, data[ds_struct.idx_to_dataset[ds_idx]][ds_struct.dok_matrix[subj,ds_idx]-1]),axis=1)\n",
    "    # compute mean and std\n",
    "    mean[subj_name] = np.mean(mtx_tmp, axis=1)\n",
    "    std[subj_name] = np.std(mtx_tmp, axis=1)\n",
    "    # standardize training data\n",
    "    for ds_idx in indices:\n",
    "        ds_name, idx_in_ds = ds_struct.idx_to_dataset[ds_idx], ds_struct.dok_matrix[subj,ds_idx]-1\n",
    "        data[ds_name][idx_in_ds] = np.nan_to_num((data[ds_name][idx_in_ds]-mean[subj_name][:,None])/std[subj_name][:,None])\n",
    "        \n",
    "# use the mean and std computed from training data to standardize testing data\n",
    "for idx, subj in enumerate(test_subj_list):\n",
    "    test_data[idx] = np.nan_to_num((test_data[idx]-mean[subj][:,None])/std[subj][:,None])\n",
    "         \n",
    "# fit the model\n",
    "model2 = MDMS(features=features, n_iter=n_iter)\n",
    "model2.fit(data, ds_struct)\n",
    "\n",
    "# transform test data\n",
    "transformed = model2.transform(test_data, test_subj_list)\n",
    "\n",
    "# zscore the transformed data\n",
    "for subj in range(len(transformed)):\n",
    "    transformed[subj] = stats.zscore(transformed[subj], axis=1, ddof=1)\n",
    "    \n",
    "# run the experiment\n",
    "accu = time_segment_matching(transformed)\n",
    "accu_mean = np.mean(accu)\n",
    "accu_se = stats.sem(accu)\n",
    "print ('Accuracy is {} +- {}'.format(accu_mean, accu_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is lower than before, so we know that the extra training data indeed transfer useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now save the fitted model into a file\n",
    "filename = 'model.pkl'\n",
    "model.save(filename)\n",
    "\n",
    "# # When we restore the model from file, we use\n",
    "# model_restored = MDMS()\n",
    "# model_restored.restore(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi node example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see ``mdms_time_segment_matching_distributed.py`` for details. Run the following line to run it with 4 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mpirun -n 4 python3 mdms_time_segment_matching_distributed.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
